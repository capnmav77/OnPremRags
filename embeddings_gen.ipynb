{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the embeddings from the text files that need to be chunked and then ingested \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ramesh.d/Documents/Projects/PP/RAG_Pipe/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import ollama\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "def load_text(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            text = file.read()\n",
    "            print(text[0:10000])\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Please make sure {0} exists.\".format(file_path))\n",
    "        return None\n",
    "    return text \n",
    "# text = load_text(\"DataLake/magma.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text chunking with window and overlap of 500 characters \n",
    "\n",
    "def chunk_text(text, window_size=300, overlap=100):\n",
    "    \"\"\"\n",
    "    Chunk text into overlapping segments of specified window size\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to chunk\n",
    "        window_size (int): Size of each chunk in characters\n",
    "        overlap (int): Number of overlapping characters between chunks\n",
    "        \n",
    "    Returns:\n",
    "        list: List of text chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        # Get chunk of window_size or remaining text if shorter\n",
    "        end = min(start + window_size, len(text))\n",
    "        chunk = text[start:end]\n",
    "        \n",
    "        # Add chunk if it's not empty\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "        # Move start position by window_size - overlap\n",
    "        start = start + window_size - overlap\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "# Create chunks with 500 char window and 100 char overlap\n",
    "# data_chunks = chunk_text(text, window_size=1000, overlap=200)\n",
    "\n",
    "# print(f\"Created {len(data_chunks)} chunks\")\n",
    "# print(f\"\\nFirst chunk sample:\\n{data_chunks[0][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model \n",
    "def load_model(model_name):\n",
    "    \"\"\"\n",
    "    Load a SentenceTransformer model from Hugging Face\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the model to load \n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "    return model\n",
    "\n",
    "# model = load_model(\"nomic-ai/CodeRankEmbed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the embedding process where the chunks that are created are embedded using the huggingface model \n",
    "\n",
    "def embed_chunks(chunks, model):\n",
    "    \"\"\"\n",
    "    Embed text chunks using the provided model\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): List of text chunks\n",
    "        model: SentenceTransformer model\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Array of embeddings\n",
    "    \"\"\"\n",
    "    sentences = chunks\n",
    "    embeddings = model.encode(sentences)\n",
    "    return embeddings\n",
    "\n",
    "# embeddings = embed_chunks(data_chunks,model)\n",
    "# print(embeddings)\n",
    "# similarities = model.similarity(embeddings, embeddings)\n",
    "# print(similarities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector store \n",
    "def create_vector_store(embeddings):\n",
    "    \"\"\"\n",
    "    Create a vector store using FAISS HNSW index\n",
    "    \n",
    "    Args:\n",
    "        embeddings (np.ndarray): Array of embeddings  \n",
    "    Returns:\n",
    "        faiss.Index: HNSW Vector store\n",
    "    \"\"\"\n",
    "    # Get the dimensionality of the embeddings\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    # Create HNSW index with proper class name\n",
    "    M = 100  # Number of connections per layer\n",
    "    vector_store = faiss.IndexHNSWFlat(dimension, M)\n",
    "    \n",
    "    # Set HNSW parameters\n",
    "    vector_store.hnsw.efConstruction = 100  # Higher = more accurate but slower construction\n",
    "    vector_store.hnsw.efSearch = 100  # Higher = more accurate but slower search\n",
    "    \n",
    "    # Make sure embeddings are float32 before adding to index\n",
    "    embeddings_32 = embeddings.astype('float32')\n",
    "    \n",
    "    # Add vectors to the index\n",
    "    vector_store.add(embeddings_32)\n",
    "    print(f\"Added {len(embeddings)} vectors to HNSW index\")\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "# vector_store = create_vector_store(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the vector store \n",
    "def store_vector_store(vector_store, file_path):\n",
    "    \"\"\"\n",
    "    Store the vector store to a file\n",
    "    \"\"\"\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(vector_store, f)\n",
    "\n",
    "def load_vector_store(file_path):\n",
    "    \"\"\"\n",
    "    Load the vector store from a file\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_vector_store(query, vector_store, model, data_chunks):\n",
    "    \"\"\"\n",
    "    Query the vector store for the most similar embeddings\n",
    "    \n",
    "    Args:\n",
    "        query (str): Query to search for\n",
    "        vector_store (faiss.IndexFlatL2): Vector store to search\n",
    "        model: SentenceTransformer model\n",
    "        \n",
    "    Returns:\n",
    "        list: List of indices of the most similar embeddings\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query])  # get the embedding\n",
    "    query_embedding = query_embedding.astype('float32')\n",
    "    distances, indices = vector_store.search(query_embedding, k=5)\n",
    "    context = \"\"\n",
    "    for i in indices[0]:\n",
    "        context += data_chunks[i] + \"\\n\"\n",
    "    return context\n",
    "\n",
    "# query = \"merging the ss tables , how?\"\n",
    "# context = query_vector_store(query, vector_store, model, data_chunks)\n",
    "# print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test out the embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now add the context to the query for the LLM\n",
    "\n",
    "def add_context_to_query(query, context):\n",
    "    \"\"\"\n",
    "    Add context to the query for the LLM\n",
    "    \n",
    "    Args:\n",
    "        query (str): Query to add context to\n",
    "        context (str): Context to add to the query\n",
    "        \n",
    "    Returns:\n",
    "        str: Query with context added\n",
    "    \"\"\"\n",
    "    return f\"Query: {query}\\nContext: {context}\"\n",
    "\n",
    "# query_with_context = add_context_to_query(query, context)\n",
    "# print(query_with_context)\n",
    "# # now use the LLM to answer the query \n",
    "\n",
    "\n",
    "# add_context_to_query(query, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send the query to the LLM \n",
    "def get_ollama_suggestions(query_with_context):\n",
    "    response = ollama.chat(model='deepseek-r1:14b', messages=[\n",
    "        {\n",
    "        'role': 'user',\n",
    "        'content': query_with_context\n",
    "        },\n",
    "    ],  options={\"temperature\": 0.8}, stream=True )\n",
    "    \n",
    "    for chunk in response:\n",
    "        print(chunk[\"message\"][\"content\"], end='', flush=True)\n",
    "        \n",
    "    return \"Streaming complete\"\n",
    "\n",
    "# get_ollama_suggestions(query_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = load_text(\"DataLake/magma.txt\")\n",
    "# # Create chunks with 500 char window and 100 char overlap\n",
    "# data_chunks = chunk_text(text, window_size=1000, overlap=200)\n",
    "\n",
    "# print(f\"Created {len(data_chunks)} chunks\")\n",
    "# print(f\"\\nFirst chunk sample:\\n{data_chunks[0][:200]}...\")\n",
    "\n",
    "# model = load_model(\"nomic-ai/CodeRankEmbed\")\n",
    "\n",
    "# embeddings = embed_chunks(data_chunks,model)\n",
    "# print(embeddings)\n",
    "# similarities = model.similarity(embeddings, embeddings)\n",
    "# print(similarities.shape)\n",
    "\n",
    "# vector_store = create_vector_store(embeddings)\n",
    "# store_vector_store(vector_store=vector_store, file_path=\"VectorStore/magma.pkl\")\n",
    "\n",
    "# vector_store = load_vector_store(file_path=\"VectorStore/magma.pkl\")\n",
    "# query = \"merging the ss tables , how?\"\n",
    "# context = query_vector_store(query, vector_store, model, data_chunks)\n",
    "# print(context)\n",
    "\n",
    "# query_with_context = add_context_to_query(query, context)\n",
    "# print(query_with_context)\n",
    "# # now use the LLM to answer the query \n",
    "\n",
    "\n",
    "# add_context_to_query(query, context)\n",
    "\n",
    "# get_ollama_suggestions(query_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# query = \"what's the use of magma \"\n",
    "# context = query_vector_store(query, vector_store, model, data_chunks)\n",
    "# print(context)\n",
    "\n",
    "# query_with_context = add_context_to_query(query, context)\n",
    "# print(query_with_context)\n",
    "# # now use the LLM to answer the query \n",
    "\n",
    "\n",
    "# add_context_to_query(query, context)\n",
    "\n",
    "# get_ollama_suggestions(query_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├── .gitignore\n",
      "├── .gitmodules\n",
      "├── LICENSE\n",
      "├── Makefile\n",
      "├── README.md\n",
      "├── TestInput.py\n",
      "├── b\n",
      "    ├── resources\n",
      "    │   ├── 1-node-template.ini\n",
      "    │   ├── 2-nodes-template.ini\n",
      "    │   ├── 20-nodes-template.ini\n",
      "    │   ├── 3-nodes-template.ini\n",
      "    │   ├── 35-nodes-template.ini\n",
      "    │   ├── 4-nodes-n1ql-index-template.ini\n",
      "    │   ├── 4-nodes-n1ql-template.ini\n",
      "    │   ├── 4-nodes-template-KV.ini\n",
      "    │   ├── 4-nodes-template-cbas-multi-cluster.ini\n",
      "    │   ├── 4-nodes-template-cbas.ini\n",
      "    │   ├── 4-nodes-template-sanity.ini\n",
      "    │   ├── 4-nodes-template.ini\n",
      "    │   ├── 5-nodes-bkrs-2clusters.ini\n",
      "    │   ├── 5-nodes-n1ql-index-template.ini\n",
      "    │   ├── 5-nodes-template.ini\n",
      "    │   ├── 6-nodes-template-cbas.ini\n",
      "    │   ├── 6-nodes-template-ce.ini\n",
      "    │   ├── 6-nodes-template-multi-cluster.ini\n",
      "    │   ├── 6-nodes-template-n1ql-xdcr.ini\n",
      "    │   ├── 6-nodes-template-n1ql.ini\n",
      "    │   ├── 6-nodes-template-xdcr.ini\n",
      "    │   ├── 6-nodes-template.ini\n",
      "    │   ├── 7-nodes-template.ini\n",
      "    │   ├── 8-nodes-template-cbas.ini\n",
      "    │   ├── 9-nodes-template.ini\n",
      "    │   ├── capella\n",
      "    │   │   ├── 3-nodes-template.ini\n",
      "    │   │   ├── goldfish-template.ini\n",
      "    │   │   ├── provisioned-template.ini\n",
      "    │   │   └── serverless-template.ini\n",
      "    │   ├── dev-3-nodes.ini\n",
      "    │   ├── dev-4-nodes.ini\n",
      "    │   ├── dev-6-nodes.ini\n",
      "    │   └── luks-nodes.ini\n",
      "    └── testdata.json\n",
      "├── build.gradle\n",
      "├── conf\n",
      "    ├── 2i\n",
      "    │   ├── cgroups_gsi.conf\n",
      "    │   └── durability_aborts.conf\n",
      "    ├── Atomicity\n",
      "    │   ├── basic.conf\n",
      "    │   ├── concurrency_basic.conf\n",
      "    │   ├── crash_test.conf\n",
      "    │   ├── diff_doc_size.conf\n",
      "    │   ├── doc_isolation.conf\n",
      "    │   ├── failover\n",
      "    │   │   ├── py-autofailover.conf\n",
      "    │   │   ├── py-diskautofailover.conf\n",
      "    │   │   └── py-newfailover.conf\n",
      "    │   ├── hooks_basic.conf\n",
      "    │   ├── multi_bucket_basic.conf\n",
      "    │   ├── param_update.conf\n",
      "    │   ├── rebalance\n",
      "    │   │   ├── rebalance_auto_retry.conf\n",
      "    │   │   ├── rebalance_in.conf\n",
      "    │   │   ├── rebalance_in_out.conf\n",
      "    │   │   ├── rebalance_out.conf\n",
      "    │   │   └── swap_rebalance.conf\n",
      "    │   └── transaction_basic.conf\n",
      "    ├── N1qlTransaction\n",
      "    │   ├── basic.conf\n",
      "    │   ├── basic_ephemeral.conf\n",
      "    │   ├── bucket_param.conf\n",
      "    │   ├── collections_failover_crud_on_collections.conf\n",
      "    │   ├── collections_rebalance_with_N1ql_txn.conf\n",
      "    │   ├── concurrency.conf\n",
      "    │   ├── concurrency_multiple_query_nodes.conf\n",
      "    │   ├── different_services.conf\n",
      "    │   ├── memory_quota.conf\n",
      "    │   ├── misc_test.conf\n",
      "    │   ├── multi_bucket.conf\n",
      "    │   ├── negative_testcase.conf\n",
      "    │   ├── os_certify_txns.conf\n",
      "    │   ├── process_crash.conf\n",
      "    │   ├── queries_use_keys.conf\n",
      "    │   ├── test_with_kvtimeout.conf\n",
      "    │   ├── test_with_prepare.conf\n",
      "    │   └── txn_durability_level.conf\n",
      "    ├── bucket\n",
      "    │   ├── bucket_level_durability.conf\n",
      "    │   ├── compaction.conf\n",
      "    │   ├── dgm.conf\n",
      "    │   ├── limits-connections.conf\n",
      "    │   ├── limits-data-size.conf\n",
      "    │   ├── limits-egress.conf\n",
      "    │   ├── limits-ingress.conf\n",
      "    │   ├── limits-ops.conf\n",
      "    │   ├── max_ttl.conf\n",
      "    │   ├── multi_bucket.conf\n",
      "    │   ├── multi_bucket_multi_durability.conf\n",
      "    │   └── replica_update.conf\n",
      "    ├── capella\n",
      "    │   ├── alert-v4-APIs.conf\n",
      "    │   ├── allowedCIDR-v4-APIs.conf\n",
      "    │   ├── appServicesOnOff.conf\n",
      "    │   ├── auditLogExports-v4-APIs.conf\n",
      "    │   ├── azureAutoExpansion-v4-APIs.conf\n",
      "    │   ├── bucket-v4-APIs.conf\n",
      "    │   ├── cluster-v4-APIs.conf\n",
      "    │   ├── clusterOnOff.conf\n",
      "    │   ├── col_v4_API-sanity.conf\n",
      "    │   ├── collection-v4-APIs.conf\n",
      "    │   ├── cp_v4_API-sanity.conf\n",
      "    │   ├── negative_tests.conf\n",
      "    │   ├── onOffSchedule-v4-APIs.conf\n",
      "    │   ├── organization-v4-APIs.conf\n",
      "    │   ├── project-v4-APIs.conf\n",
      "    │   ├── sampleBucket-v4-APIs.conf\n",
      "    │   ├── scope-v4-APIs.conf\n",
      "    │   └── throttle_tests.conf\n",
      "    ├── cas_test\n",
      "    │   └── castest.conf\n",
      "    ├── cbas\n",
      "    │   ├── magma\n",
      "    │   │   ├── py-cbas-collection-failover.conf\n",
      "    │   │   ├── py-cbas-collection-multi-bucket.conf\n",
      "    │   │   ├── py-cbas-collection-rebalance.conf\n",
      "    │   │   └── py-cbas-collection.conf\n",
      "    │   ├── py-cbas-audit.conf\n",
      "    │   ├── py-cbas-backup-restore.conf\n",
      "    │   ├── py-cbas-bucket-operations.conf\n",
      "    │   ├── py-cbas-bug-automation.conf\n",
      "    │   ├── py-cbas-capella-automation.conf\n",
      "    │   ├── py-cbas-cbo.conf\n",
      "    │   ├── py-cbas-cgroup.conf\n",
      "    │   ├── py-cbas-collection-failover.conf\n",
      "    │   ├── py-cbas-collection-rebalance.conf\n",
      "    │   ├── py-cbas-collections.conf\n",
      "    │   ├── py-cbas-error-response.conf\n",
      "    │   ├── py-cbas-external-datasets-AWS-parquet.conf\n",
      "    │   ├── py-cbas-external-links-AWS.conf\n",
      "    │   ├── py-cbas-external-links-azure_blob.conf\n",
      "    │   ├── py-cbas-high-availability.conf\n",
      "    │   ├── py-cbas-ip-address-family.conf\n",
      "    │   ├── py-cbas-metadata-replication.conf\n",
      "    │   ├── py-cbas-multiple_ca.conf\n",
      "    │   ├── py-cbas-os-certify.conf\n",
      "    │   ├── py-cbas-remote-links-datasets.conf\n",
      "    │   ├── py-cbas-secondary-indexes.conf\n",
      "    │   ├── py-cbas-system-event-logs.conf\n",
      "    │   └── py-cbas-udf-management.conf\n",
      "    ├── collections\n",
      "    │   ├── app.conf\n",
      "    │   ├── bucket_flush.conf\n",
      "    │   ├── bucket_param_update.conf\n",
      "    │   ├── bucket_warmup.conf\n",
      "    │   ├── cas.conf\n",
      "    │   ├── collections_auto_retry_failed_rebalance.conf\n",
      "    │   ├── collections_autofailover.conf\n",
      "    │   ├── collections_autoreprovision_0_replica.conf\n",
      "    │   ├── collections_dgm_steady.conf\n",
      "    │   ├── collections_diskautofailover.conf\n",
      "    │   ├── collections_drop_recreate_rebalance.conf\n",
      "    │   ├── collections_failover.conf\n",
      "    │   ├── collections_failover_crud_on_collections.conf\n",
      "    │   ├── collections_failover_crud_on_collections_dgm.conf\n",
      "    │   ├── collections_failover_dgm.conf\n",
      "    │   ├── collections_multinodeautofailover.conf\n",
      "    │   ├── collections_negative_tc.conf\n",
      "    │   ├── collections_network_split.conf\n",
      "    │   ├── collections_quorum_loss_failover.conf\n",
      "    │   ├── collections_rebalance.conf\n",
      "    │   ├── collections_rebalance_crud_on_collections.conf\n",
      "    │   ├── collections_rebalance_crud_on_collections_dgm.conf\n",
      "    │   ├── collections_rebalance_dgm.conf\n",
      "    │   ├── collections_start_stop_tests.conf\n",
      "    │   ├── collections_with_ttl.conf\n",
      "    │   ├── compaction.conf\n",
      "    │   ├── dcp_oso_backfill.conf\n",
      "    │   ├── document_keys.conf\n",
      "    │   ├── durability_failures.conf\n",
      "    │   ├── durability_success.conf\n",
      "    │   ├── process_crash.conf\n",
      "    │   ├── rollback.conf\n",
      "    │   ├── sanity.conf\n",
      "    │   ├── sdk_exception_tests.conf\n",
      "    │   ├── steady_state.conf\n",
      "    │   └── steady_state_compression.conf\n",
      "    ├── columnar\n",
      "    │   ├── columnar_rbac_cloud.conf\n",
      "    │   ├── copy_into_standalone_collection_from_S3.conf\n",
      "    │   ├── copy_to_kv.conf\n",
      "    │   ├── copy_to_s3.conf\n",
      "    │   ├── dynamic_prefixes.conf\n",
      "    │   ├── external_links_datasets_S3.conf\n",
      "    │   ├── goldfish_volume.conf\n",
      "    │   ├── insert_upsert_delete_standalone_collection.conf\n",
      "    │   ├── legacy_cbas\n",
      "    │   │   ├── py-cbas-cbo.conf\n",
      "    │   │   ├── py-cbas-collections.conf\n",
      "    │   │   ├── py-cbas-external-links-AWS.conf\n",
      "    │   │   ├── py-cbas-remote-links-datasets.conf\n",
      "    │   │   ├── py-cbas-secondary-indexes.conf\n",
      "    │   │   ├── py-cbas-standalone-collection-external-link-s3.conf\n",
      "    │   │   └── py-cbas-udf-management.conf\n",
      "    │   ├── on_off.conf\n",
      "    │   ├── onprem\n",
      "    │   │   ├── build_verification_sanity.conf\n",
      "    │   │   ├── columnar_rbac_onprem.conf\n",
      "    │   │   └── copy_to_kv.conf\n",
      "    │   ├── remote_links_datasets.conf\n",
      "    │   ├── sample_sirius_test.conf\n",
      "    │   ├── scaling_operations.conf\n",
      "    │   ├── security.conf\n",
      "    │   └── standalone_collection.conf\n",
      "    ├── dcp\n",
      "    │   └── dcp_new_test.conf\n",
      "    ├── ent_bkrs\n",
      "    │   └── bkrs_transactions.conf\n",
      "    ├── ep_engine\n",
      "    │   ├── basic_ops.conf\n",
      "    │   ├── cas_durability_failures.conf\n",
      "    │   ├── cas_test.conf\n",
      "    │   ├── documentkeys.conf\n",
      "    │   ├── durability_failures.conf\n",
      "    │   ├── durability_success.conf\n",
      "    │   ├── durability_timeouts.conf\n",
      "    │   ├── large_docs.conf\n",
      "    │   ├── ooo_returns.conf\n",
      "    │   ├── process_crash.conf\n",
      "    │   ├── sub_doc_failures.conf\n",
      "    │   ├── sub_doc_success.conf\n",
      "    │   ├── subdoc_xattr.conf\n",
      "    │   ├── syncwrite_failures_with_creates.conf\n",
      "    │   ├── syncwrite_failures_with_deletes.conf\n",
      "    │   ├── syncwrite_failures_with_upserts.conf\n",
      "    │   ├── tombstones_a.conf\n",
      "    │   ├── tombstones_b.conf\n",
      "    │   └── xattr_tests.conf\n",
      "    ├── eventing\n",
      "    │   ├── eventing_cgroup.conf\n",
      "    │   └── eventing_durability_aborts.conf\n",
      "    ├── failover\n",
      "    │   ├── concurrent_failovers.conf\n",
      "    │   ├── failover_with_durability.conf\n",
      "    │   ├── fast_failover.conf\n",
      "    │   ├── py-auto-failover-server-stop.conf\n",
      "    │   ├── py-autofailover-buckets.conf\n",
      "    │   ├── py-autofailover-firewall.conf\n",
      "    │   ├── py-autofailover-memcached.conf\n",
      "    │   ├── py-autofailover-network-split.conf\n",
      "    │   ├── py-autofailover.conf\n",
      "    │   ├── py-multinodefailover.conf\n",
      "    │   ├── py-multinodefailover_rebalance.conf\n",
      "    │   ├── py-negativefailover.conf\n",
      "    │   ├── py-newfailover.conf\n",
      "    │   └── py-servergroup-failover.conf\n",
      "    ├── fts\n",
      "    │   └── fts_durability_aborts.conf\n",
      "    ├── guardrails\n",
      "    │   ├── bucket_guardrails.conf\n",
      "    │   ├── disk_usage_guardrails.conf\n",
      "    │   ├── guardrails_with_migration.conf\n",
      "    │   ├── guardrails_with_upgrades.conf\n",
      "    │   ├── max_data_guardrails.conf\n",
      "    │   └── rr_guardrails.conf\n",
      "    ├── history_retention\n",
      "    │   ├── rebalance_test.conf\n",
      "    │   └── steady_state.conf\n",
      "    ├── magma\n",
      "    │   ├── 10gb_per_vbucket.conf\n",
      "    │   ├── basic_cruds_del.conf\n",
      "    │   ├── basic_cruds_get.conf\n",
      "    │   ├── basic_cruds_low_RAM.conf\n",
      "    │   ├── basic_cruds_set.conf\n",
      "    │   ├── basic_cruds_upsert.conf\n",
      "    │   ├── basic_upsert_delete.conf\n",
      "    │   ├── cdc_magma_failures.conf\n",
      "    │   ├── cdc_steady_state.conf\n",
      "    │   ├── collections_failover.conf\n",
      "    │   ├── collections_failover_crud_on_collections.conf\n",
      "    │   ├── collections_rebalance.conf\n",
      "    │   ├── collections_rebalance_crud_on_collections.conf\n",
      "    │   ├── compaction.conf\n",
      "    │   ├── crash_recovery.conf\n",
      "    │   ├── crash_recovery_low_RAM.conf\n",
      "    │   ├─\n",
      "Created 112470 chunks\n",
      "\n",
      "First chunk sample:\n",
      "├── .gitignore\n",
      "├── .gitmodules\n",
      "├── LICENSE\n",
      "├── Makefile\n",
      "├── README.md\n",
      "├── TestInput.py\n",
      "├── b\n",
      "    ├── resources\n",
      "    │   ├── 1-node-template.ini\n",
      "    │   ├── 2-nodes-template.ini\n",
      "    │   ├── 20-nodes-tem...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.08511648  0.6917985  -0.5125859  ...  1.3397381  -0.8910495\n",
      "  -1.3364283 ]\n",
      " [-0.45843035  0.7126437  -0.32616398 ...  0.80734634 -1.1409655\n",
      "  -1.0520488 ]\n",
      " [-0.9916344   0.60744303 -0.4247943  ...  0.3345994  -1.1995372\n",
      "  -0.9208341 ]\n",
      " ...\n",
      " [ 0.26201653 -0.3331136  -0.04144606 ... -2.0046778  -0.5560208\n",
      "  -1.5676023 ]\n",
      " [-0.57169724  0.34139484 -0.38562724 ... -1.7988298   0.01153683\n",
      "   0.2464078 ]\n",
      " [ 0.16628617 -0.78377783 -1.1569986  ...  0.3038962  -0.10967173\n",
      "   0.24487083]]\n",
      "torch.Size([112470, 112470])\n"
     ]
    }
   ],
   "source": [
    "text = load_text(\"DataLake/TAF.txt\")\n",
    "# Create chunks with 500 char window and 100 char overlap\n",
    "data_chunks = chunk_text(text, window_size=300, overlap=100)\n",
    "\n",
    "print(f\"Created {len(data_chunks)} chunks\")\n",
    "print(f\"\\nFirst chunk sample:\\n{data_chunks[0][:200]}...\")\n",
    "\n",
    "model = load_model(\"nomic-ai/CodeRankEmbed\")\n",
    "\n",
    "embeddings = embed_chunks(data_chunks,model)\n",
    "print(embeddings)\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities.shape)\n",
    "\n",
    "vector_store = create_vector_store(embeddings)\n",
    "store_vector_store(vector_store=vector_store, file_path=\"VectorStore/TAF_HNSW.pkl\")\n",
    "\n",
    "vector_store = load_vector_store(file_path=\"VectorStore/TAF_HNSW.pkl\")\n",
    "query = \"what's the use of check history_retention function in magma?\"\n",
    "context = query_vector_store(query, vector_store, model, data_chunks)\n",
    "print(context)\n",
    "\n",
    "query_with_context = add_context_to_query(query, context)\n",
    "print(query_with_context)\n",
    "# now use the LLM to answer the query \n",
    "\n",
    "\n",
    "add_context_to_query(query, context)\n",
    "\n",
    "get_ollama_suggestions(query_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oteMachineShellConnection(self.server)\n",
      " 45 |         output, error = remote_client.execute_command(self.command)\n",
      " 46 |         print(self.server.ip)\n",
      " 47 |         print(\"\\n\".join(output))\n",
      " 48 |         print(\"\\n\".join(error))\n",
      " 49 |         remote_client.disconnect()\n",
      " 50 | \n",
      " 51 | \n",
      " 52 | class ScriptRunner(object):\n",
      " 53 |     def __init__(self, server, script):\n",
      " 54 |         self.server = server\n",
      " 55 |         with open(script) as  f:\n",
      " 56 |             self.script_content = f.read()\n",
      " 57 |         self.script_name = \"/tmp/\" + str(uuid.uuid4())\n",
      " 58 | \n",
      " 59 |     def run(self):\n",
      " 60 |         remote_client = RemoteMachineShellConnection(self.server)\n",
      " 61 |         remote_client.create_file(self.script_name, self.script_content)\n",
      " 62 |         output, error = remote_client.execute_command(\n",
      " 63 |             \"chmod 777 {0} ; {0} ; rm -f {0}\".format(self.script_name))\n",
      " 64 |         print(self.server.ip)\n",
      " 65 |         print(\"\\n\".join(output))\n",
      " 66 |         print(\"\\n\".join(error))\n",
      " 67 |         remote\n",
      "ocess\n",
      " 122 |         else:\n",
      " 123 |             processes = self.remote_shell.get_running_processes()\n",
      " 124 |             for process in processes:\n",
      " 125 |                 if process.name == process_name:\n",
      " 126 |                     return process\n",
      " 127 |                 elif process_name in process.args:\n",
      " 128 |                     return process\n",
      " 129 |             return None\n",
      " 130 | \n",
      " 131 | \n",
      " 132 | class RemoteMachineShellConnection:\n",
      " 133 |     connections = 0\n",
      " 134 |     disconnections = 0\n",
      " 135 | \n",
      " 136 |     def __init__(self, serverInfo):\n",
      " 137 |         RemoteMachineShellConnection.connections += 1\n",
      " 138 |         self.jsch = None\n",
      " 139 |         self.session = None\n",
      " 140 |         self.input = TestInput.TestInputParser.get_test_input(sys.argv)\n",
      " 141 |         self.log = logger.get(\"infra\")\n",
      " 142 |         self.test_log = logger.get(\"test\")\n",
      " 143 | \n",
      " 144 |         self.ip = serverInfo.ip\n",
      " 145 |         self.username = serverInfo.ssh_username\n",
      " 146 |         self.password = serverInfo.ssh_password\n",
      "ShellConnection\n",
      " 8 | from BucketOperations_Rest import BucketHelper as BucketHelperRest\n",
      " 9 | from bucket import Bucket\n",
      "10 | \n",
      "11 | \n",
      "12 | class BucketHelper(BucketHelperRest):\n",
      "13 |     def __init__(self, server, username=\"Administrator\", password=\"password\"):\n",
      "14 |         super(BucketHelper, self).__init__(server)\n",
      "15 |         self.server = server\n",
      "16 |         self.username = username\n",
      "17 |         self.password = password\n",
      "18 |         self.cb_cli = None\n",
      "19 | \n",
      "20 |     def __use_shell(self, cli_function):\n",
      "21 |         def wrapper():\n",
      "22 |             shell = RemoteMachineShellConnection(self.server)\n",
      "23 |             self.cb_cli = CbCli(shell, self.username, self.password)\n",
      "24 |             try:\n",
      "25 |                 output = cli_function()\n",
      "26 |             except Exception as e:\n",
      "27 |                 shell.disconnect()\n",
      "28 |                 raise e\n",
      "29 |             shell.disconnect()\n",
      "30 |             return output\n",
      "31 |         return wrapper\n",
      "32 | \n",
      "33 |     @__use_shell\n",
      "34 |     def create_buck\n",
      "RemoteMachineShellConnection(self.servers[0])\n",
      "  83 |         info = shell.extract_remote_info().type.lower()\n",
      "  84 |         self.root_path = Linux.ROOT_PATH\n",
      "  85 |         self.wget = \"wget\"\n",
      "  86 |         self.os_name = \"linux\"\n",
      "  87 |         self.tmp_path = \"/tmp/\"\n",
      "  88 |         self.long_help_flag = \"--help\"\n",
      "  89 |         self.short_help_flag = \"-h\"\n",
      "  90 |         self.cygwin_bin_path = \"\"\n",
      "  91 |         self.enable_firewal = False\n",
      "  92 |         self.rfc3339_date = \"date +%s --date='{0} seconds' | \".format(self.replace_ttl_with) + \\\n",
      "  93 |                                 \"xargs -I {} date --date='@{}' --rfc-3339=seconds | \"\\\n",
      "  94 |                                 \"sed 's/ /T/'\"\n",
      "  95 |         self.seconds_with_ttl = \"date +%s --date='{0} seconds'\".format(self.replace_ttl_with)\n",
      "  96 |         if info == Linux.NAME:\n",
      "  97 |             if self.nonroot:\n",
      "  98 |                 base_path = \"/home/%s\" % self.cluster.master.ssh_username\n",
      "  99 |                 self.database_path = \"%s%s\" \n",
      "tils.capella_utils.dedicated import CapellaUtils\n",
      "  11 | from TestInput import TestInputSingleton\n",
      "  12 | from platform_utils.remote.remote_util import RemoteMachineShellConnection\n",
      "  13 | \n",
      "  14 | \n",
      "  15 | class ServerInfo:\n",
      "  16 |     def __init__(self,\n",
      "  17 |                  ip,\n",
      "  18 |                  port,\n",
      "  19 |                  ssh_username,\n",
      "  20 |                  ssh_password,\n",
      "  21 |                  memcached_port,\n",
      "  22 |                  ssh_key=''):\n",
      "  23 |         self.ip = ip\n",
      "  24 |         self.ssh_username = ssh_username\n",
      "  25 |         self.ssh_password = ssh_password\n",
      "  26 |         self.port = port\n",
      "  27 |         self.ssh_key = ssh_key\n",
      "  28 |         self.memcached_port = memcached_port\n",
      "  29 |         self.type = None\n",
      "  30 |         self.remote_info = None\n",
      "  31 | \n",
      "  32 | \n",
      "  33 | class SecurityTest(SecurityBase):\n",
      "  34 |     SLAVE_HOST = ServerInfo('127.0.0.1', 22, 'root', 'couchbase', 18091)\n",
      "  35 | \n",
      "  36 |     def setUp(self):\n",
      "  37 |         try:\n",
      "  38 |             SecurityBa\n",
      "\n",
      "Query: How to use RemoteShellConnection in test and give me usage\n",
      "Context: oteMachineShellConnection(self.server)\n",
      " 45 |         output, error = remote_client.execute_command(self.command)\n",
      " 46 |         print(self.server.ip)\n",
      " 47 |         print(\"\\n\".join(output))\n",
      " 48 |         print(\"\\n\".join(error))\n",
      " 49 |         remote_client.disconnect()\n",
      " 50 | \n",
      " 51 | \n",
      " 52 | class ScriptRunner(object):\n",
      " 53 |     def __init__(self, server, script):\n",
      " 54 |         self.server = server\n",
      " 55 |         with open(script) as  f:\n",
      " 56 |             self.script_content = f.read()\n",
      " 57 |         self.script_name = \"/tmp/\" + str(uuid.uuid4())\n",
      " 58 | \n",
      " 59 |     def run(self):\n",
      " 60 |         remote_client = RemoteMachineShellConnection(self.server)\n",
      " 61 |         remote_client.create_file(self.script_name, self.script_content)\n",
      " 62 |         output, error = remote_client.execute_command(\n",
      " 63 |             \"chmod 777 {0} ; {0} ; rm -f {0}\".format(self.script_name))\n",
      " 64 |         print(self.server.ip)\n",
      " 65 |         print(\"\\n\".join(output))\n",
      " 66 |         print(\"\\n\".join(error))\n",
      " 67 |         remote\n",
      "ocess\n",
      " 122 |         else:\n",
      " 123 |             processes = self.remote_shell.get_running_processes()\n",
      " 124 |             for process in processes:\n",
      " 125 |                 if process.name == process_name:\n",
      " 126 |                     return process\n",
      " 127 |                 elif process_name in process.args:\n",
      " 128 |                     return process\n",
      " 129 |             return None\n",
      " 130 | \n",
      " 131 | \n",
      " 132 | class RemoteMachineShellConnection:\n",
      " 133 |     connections = 0\n",
      " 134 |     disconnections = 0\n",
      " 135 | \n",
      " 136 |     def __init__(self, serverInfo):\n",
      " 137 |         RemoteMachineShellConnection.connections += 1\n",
      " 138 |         self.jsch = None\n",
      " 139 |         self.session = None\n",
      " 140 |         self.input = TestInput.TestInputParser.get_test_input(sys.argv)\n",
      " 141 |         self.log = logger.get(\"infra\")\n",
      " 142 |         self.test_log = logger.get(\"test\")\n",
      " 143 | \n",
      " 144 |         self.ip = serverInfo.ip\n",
      " 145 |         self.username = serverInfo.ssh_username\n",
      " 146 |         self.password = serverInfo.ssh_password\n",
      "ShellConnection\n",
      " 8 | from BucketOperations_Rest import BucketHelper as BucketHelperRest\n",
      " 9 | from bucket import Bucket\n",
      "10 | \n",
      "11 | \n",
      "12 | class BucketHelper(BucketHelperRest):\n",
      "13 |     def __init__(self, server, username=\"Administrator\", password=\"password\"):\n",
      "14 |         super(BucketHelper, self).__init__(server)\n",
      "15 |         self.server = server\n",
      "16 |         self.username = username\n",
      "17 |         self.password = password\n",
      "18 |         self.cb_cli = None\n",
      "19 | \n",
      "20 |     def __use_shell(self, cli_function):\n",
      "21 |         def wrapper():\n",
      "22 |             shell = RemoteMachineShellConnection(self.server)\n",
      "23 |             self.cb_cli = CbCli(shell, self.username, self.password)\n",
      "24 |             try:\n",
      "25 |                 output = cli_function()\n",
      "26 |             except Exception as e:\n",
      "27 |                 shell.disconnect()\n",
      "28 |                 raise e\n",
      "29 |             shell.disconnect()\n",
      "30 |             return output\n",
      "31 |         return wrapper\n",
      "32 | \n",
      "33 |     @__use_shell\n",
      "34 |     def create_buck\n",
      "RemoteMachineShellConnection(self.servers[0])\n",
      "  83 |         info = shell.extract_remote_info().type.lower()\n",
      "  84 |         self.root_path = Linux.ROOT_PATH\n",
      "  85 |         self.wget = \"wget\"\n",
      "  86 |         self.os_name = \"linux\"\n",
      "  87 |         self.tmp_path = \"/tmp/\"\n",
      "  88 |         self.long_help_flag = \"--help\"\n",
      "  89 |         self.short_help_flag = \"-h\"\n",
      "  90 |         self.cygwin_bin_path = \"\"\n",
      "  91 |         self.enable_firewal = False\n",
      "  92 |         self.rfc3339_date = \"date +%s --date='{0} seconds' | \".format(self.replace_ttl_with) + \\\n",
      "  93 |                                 \"xargs -I {} date --date='@{}' --rfc-3339=seconds | \"\\\n",
      "  94 |                                 \"sed 's/ /T/'\"\n",
      "  95 |         self.seconds_with_ttl = \"date +%s --date='{0} seconds'\".format(self.replace_ttl_with)\n",
      "  96 |         if info == Linux.NAME:\n",
      "  97 |             if self.nonroot:\n",
      "  98 |                 base_path = \"/home/%s\" % self.cluster.master.ssh_username\n",
      "  99 |                 self.database_path = \"%s%s\" \n",
      "tils.capella_utils.dedicated import CapellaUtils\n",
      "  11 | from TestInput import TestInputSingleton\n",
      "  12 | from platform_utils.remote.remote_util import RemoteMachineShellConnection\n",
      "  13 | \n",
      "  14 | \n",
      "  15 | class ServerInfo:\n",
      "  16 |     def __init__(self,\n",
      "  17 |                  ip,\n",
      "  18 |                  port,\n",
      "  19 |                  ssh_username,\n",
      "  20 |                  ssh_password,\n",
      "  21 |                  memcached_port,\n",
      "  22 |                  ssh_key=''):\n",
      "  23 |         self.ip = ip\n",
      "  24 |         self.ssh_username = ssh_username\n",
      "  25 |         self.ssh_password = ssh_password\n",
      "  26 |         self.port = port\n",
      "  27 |         self.ssh_key = ssh_key\n",
      "  28 |         self.memcached_port = memcached_port\n",
      "  29 |         self.type = None\n",
      "  30 |         self.remote_info = None\n",
      "  31 | \n",
      "  32 | \n",
      "  33 | class SecurityTest(SecurityBase):\n",
      "  34 |     SLAVE_HOST = ServerInfo('127.0.0.1', 22, 'root', 'couchbase', 18091)\n",
      "  35 | \n",
      "  36 |     def setUp(self):\n",
      "  37 |         try:\n",
      "  38 |             SecurityBa\n",
      "\n",
      "<think>\n",
      "Okay, I need to figure out how to use RemoteShellConnection based on the provided code snippets. Let me start by looking through the context given.\n",
      "\n",
      "I see that RemoteMachineShellConnection is a class defined in lines 132-146 and elsewhere. It's used for connecting to remote machines via SSH. The __init__ method initializes it with server info, including IP, username, and password.\n",
      "\n",
      "Looking at how it's used in other parts of the code, like in line 45 where a client executes a command and prints output and error. Another example is in the ScriptRunner class where they create a file on the remote machine, run a script, and then disconnect.\n",
      "\n",
      "So, to use RemoteShellConnection, I should probably:\n",
      "\n",
      "1. Import the necessary modules.\n",
      "2. Create an instance by providing server info (IP, username, password).\n",
      "3. Use methods like execute_command to run commands.\n",
      "4. Handle output and errors appropriately.\n",
      "5. Disconnect after use to manage resources.\n",
      "\n",
      "I might also need to handle exceptions or ensure connections are closed properly, maybe using try-finally blocks or context managers if available.\n",
      "\n",
      "Let me outline the steps I think are needed:\n",
      "\n",
      "- Import RemoteMachineShellConnection from its module.\n",
      "- Create a ServerInfo object with the target IP, username, and password.\n",
      "- Instantiate RemoteMachineShellConnection with this server info.\n",
      "- Use execute_command to run the desired command.\n",
      "- Capture output and errors for logging or processing.\n",
      "- Disconnect after finishing.\n",
      "\n",
      "I should also check if there are any specific parameters or flags needed when initializing, but from the context, it seems the constructor requires serverInfo which includes all necessary details.\n",
      "\n",
      "Potential issues I might face: connection timeouts, authentication failures, or handling SSH keys if required. But based on the code, password-based auth is supported.\n",
      "\n",
      "I think that's a good start. Now, let me put together an example of how to use RemoteShellConnection in a test.\n",
      "</think>\n",
      "\n",
      "To utilize `RemoteMachineShellConnection` effectively in your tests, follow these steps:\n",
      "\n",
      "### 1. Import Necessary Modules\n",
      "Ensure you import the `RemoteMachineShellConnection` class from its respective module.\n",
      "\n",
      "```python\n",
      "from platform_utils.remote.remote_util import RemoteMachineShellConnection\n",
      "```\n",
      "\n",
      "### 2. Define Server Information\n",
      "Create a `ServerInfo` object with the required details for connecting to the remote machine.\n",
      "\n",
      "```python\n",
      "server_info = ServerInfo(\n",
      "    ip='remote_machine_ip',\n",
      "    port=22,\n",
      "    ssh_username='username',\n",
      "    ssh_password='password'\n",
      ")\n",
      "```\n",
      "\n",
      "### 3. Establish Remote Connection\n",
      "Instantiate `RemoteMachineShellConnection` using the server information.\n",
      "\n",
      "```python\n",
      "with RemoteMachineShellConnection(server_info) as shell:\n",
      "    # Execute commands within this block to ensure proper cleanup\n",
      "    pass\n",
      "```\n",
      "\n",
      "Using a `with` statement ensures that the connection is properly closed after execution, even if an error occurs.\n",
      "\n",
      "### 4. Execute Commands\n",
      "Within the context manager, execute the desired command using the `execute_command` method.\n",
      "\n",
      "```python\n",
      "_, stdout, stderr = shell.execute_command('ls -l')\n",
      "print(\"Output:\", stdout.read())\n",
      "print(\"Error:\", stderr.read())\n",
      "```\n",
      "\n",
      "The `execute_command` method returns three file-like objects: stdin, stdout, and stderr. You can use these to capture command outputs.\n",
      "\n",
      "### 5. Handle Edge Cases\n",
      "Always ensure that the connection is closed properly to avoid resource leaks. Using a context manager (`with` statement) handles this automatically.\n",
      "\n",
      "### Complete Example\n",
      "\n",
      "Here's a complete example integrating all the steps:\n",
      "\n",
      "```python\n",
      "from platform_utils.remote.remote_util import RemoteMachineShellConnection\n",
      "from TestInput import TestInputSingleton\n",
      "\n",
      "# Initialize Test Input Singleton if necessary\n",
      "TestInput = TestInputSingleton()\n",
      "\n",
      "def test_remote_connection():\n",
      "    # Define server information\n",
      "    server_info = ServerInfo(\n",
      "        ip='remote_machine_ip',\n",
      "        port=22,\n",
      "        ssh_username='username',\n",
      "        ssh_password='password'\n",
      "    )\n",
      "\n",
      "    # Use the context manager to handle connection and cleanup\n",
      "    with RemoteMachineShellConnection(server_info) as shell:\n",
      "        # Execute a command\n",
      "        _, stdout, stderr = shell.execute_command('ls -l')\n",
      "\n",
      "        # Read output and errors\n",
      "        print(\"Output:\", stdout.read().decode())\n",
      "        print(\"Error:\", stderr.read().decode())\n",
      "\n",
      "        # Check for errors\n",
      "        if stderr.read():\n",
      "            raise Exception(f\"Command execution failed with error: {stderr}\")\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    test_remote_connection()\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "- **Resource Management**: Using a `with` statement ensures that the connection is closed automatically, even if an exception occurs.\n",
      "- **Output Handling**: The stdout and stderr are returned as file-like objects. Reading their content using `.read()` gives the output. Decoding might be necessary depending on your system's locale.\n",
      "- **Error Handling**: Always check for errors in command execution. If `stderr` is not empty, it indicates a failure.\n",
      "\n",
      "By following these steps, you can effectively use `RemoteMachineShellConnection` to execute commands and manage remote resources in your tests."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Streaming complete'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How to use RemoteShellConnection in test and give me usage\"\n",
    "context = query_vector_store(query, vector_store, model, data_chunks)\n",
    "print(context)\n",
    "\n",
    "query_with_context = add_context_to_query(query, context)\n",
    "print(query_with_context)\n",
    "# now use the LLM to answer the query \n",
    "\n",
    "\n",
    "add_context_to_query(query, context)\n",
    "\n",
    "get_ollama_suggestions(query_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
