{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the embeddings from the text files that need to be chunked and then ingested \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABSTRACT\n",
      "We present Magma, a write-optimized high data density key-value\n",
      "storage engine used in the Couchbase NoSQL distributed docu-\n",
      "ment database. Today’s write-heavy data-intensive applications\n",
      "like ad-serving, internet-of-things, messaging, and online gaming,\n",
      "generate massive amounts of data. As a result, the requirement\n",
      "for storing and retrieving large volumes of data has grown rapidly.\n",
      "Distributed databases that can scale out horizontally by adding\n",
      "more nodes can be used to serve the requirements of these internet-\n",
      "scale applications. To maintain a reasonable cost of ownership, we\n",
      "need to improve storage eciency in handling large data volumes\n",
      "per node, such that we don’t have to rely on adding more nodes.\n",
      "Our current generation storage engine, Couchstore is based on a\n",
      "log-structured append-only copy-on-write B+Tree architecture. To\n",
      "make substantial improvements to support higher data density and\n",
      "write throughput, we needed a storage engine architecture that\n",
      "lowers write amplication and avoids compaction operations that\n",
      "rewrite the whole database les periodically.\n",
      "We introduce Magma, a hybrid key-value storage engine that\n",
      "combines LSM Trees and a segmented log approach from log-\n",
      "structured le systems. We present a novel approach to perform-\n",
      "ing garbage collection of stale document versions avoiding index\n",
      "lookup during log segment compaction. This is the key to achieving\n",
      "storage eciency for Magma and eliminates the need for random\n",
      "I/Os during compaction. Magma oers signicantly lower write\n",
      "amplication, scalable incremental compaction, and lower space\n",
      "amplication while not regressing the read amplication. Through\n",
      "the eciency improvements, we improved the single machine data\n",
      "density supported by the Couchbase Server by 3.3x and lowered\n",
      "the memory requirement by 10x, thereby reducing the total cost\n",
      "of ownership up to 10x. Our evaluation results show that Magma\n",
      "outperforms Couchstore and RocksDB in write-heavy workloads.\n",
      "PVLDB Reference Format:\n",
      "Sarath Lakshman, Apaar Gupta, Rohan Suri, Scott Lashley, John Liang,\n",
      "Srinath Duvuru, and Ravi Mayuram. Magma: A High Data Density Storage\n",
      "Engine Used in Couchbase. PVLDB, 15(12): 3496-3508, 2022.\n",
      "doi:10.14778/3554821.3554839\n",
      "1 INTRODUCTION\n",
      "Modern-day internet-scale interactive applications generate huge\n",
      "amounts of data through user engagements. These data-intensive\n",
      "applications like ad-serving, internet-of-things, messaging, and on-\n",
      "line gaming are real-time and write-heavy, requiring large storage\n",
      "capacity and high transaction throughput. As a result, distributed\n",
      "databases that can scale horizontally have become an integral part\n",
      "of the modern data infrastructure stack that needs to operate at\n",
      "scale. The rapid growth of data volumes due to the digital wave\n",
      "has introduced challenges from a manageability and storage cost\n",
      "perspective. These problems have only grown despite the cost of\n",
      "computing and storage hardware like memory and ash dropping\n",
      "because the cost reduction has not kept up with the growth of\n",
      "data. The high throughput and storage capacity can be achieved\n",
      "by scaling out the distributed database by adding more nodes. To\n",
      "maintain a reasonable cost of ownership, we need to improve stor-\n",
      "age eciency in handling large data volumes per node, such that\n",
      "we don’t have to rely on adding more nodes.\n",
      "Under the hood, a single node of the distributed database depends\n",
      "on a persistent key-value storage engine for durable storage and\n",
      "retrieval of the database records. B+Trees [7] and Log structured\n",
      "merge trees [26] are two popular access methods for implementing\n",
      "persistent key-value storage engines. B+Tree is a read-optimized\n",
      "data structure while LSM Tree is write-optimized. Both of these\n",
      "data structures can be found in popular distributed databases like\n",
      "Couchbase, Cassandra, MongoDB, CockroachDB, etc. The eciency\n",
      "and performance of I/O intensive index structures are essentially a\n",
      "balance among three properties. Write amplication, read ampli-\n",
      "cation, and space amplication (RUM Conjecture) [4]. We cannot\n",
      "achieve write-optimized, read-optimized, and space-optimized per-\n",
      "sistent index structures all at the same time. Write amplication\n",
      "denes the ratio of the amount of data written to disk for every byte\n",
      "of write to the storage engine. Read amplication is the number\n",
      "of reads issued to the disk for every read operation of the storage\n",
      "engine. Space amplication is the ratio of the amount of data stored\n",
      "on a disk to the user input data size.\n",
      "Key Challenges with High Data Density. We start by identify-\n",
      "ing the challenges faced by our append-only copy-on-write B+Tree\n",
      "based storage engine to sustain high write throughput with a large\n",
      "volume of data per node with a database size to memory ratio of\n",
      "100x.\n",
      "Slow Writes. Updates in a copy-on-write B+Tree are done as\n",
      "read-modify-write, requiring random read I/Os. As the density\n",
      "increases, reads incur large cache misses for the B+Tree pages.\n",
      "Keys are spread out in a large key range distribution, and hence\n",
      "larger B+Tree. The opportunity for deduplication before writing\n",
      "and amortization of page rewrites due to large batches reduces,\n",
      "thereby increasing the write amplication. Write latency increases\n",
      "and throughput drops.\n",
      "Compaction Challenges. When the database becomes frag-\n",
      "mented, a compaction operation needs to be performed to limit\n",
      "space amplication. Compaction performs a full database rewrite\n",
      "This work is licensed under the Creative Commons BY-NC-ND 4.0 International\n",
      "License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\n",
      "this license. For any use beyond those covered by this license, obtain permission by\n",
      "emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\n",
      "licensed to the VLDB Endowment.\n",
      "Proceedings of the VLDB Endowment, Vol. 15, No. 12 ISSN 2150-8097.\n",
      "doi:10.14778/3554821.3554839\n",
      "3496\n",
      "by copying live documents to a new le and building the B+Tree\n",
      "indexes, taking time proportional to the database size. After copy-\n",
      "ing, it has to run catchup to replay the extra changes that came\n",
      "in during the copying. This introduces high write amplication.\n",
      "Writes can only run at the speed of single-threaded compaction\n",
      "per DB le. Even though we have several DB les per node, as the\n",
      "density increases, the size of individual les increases. Full DB le\n",
      "rewrite and longer duration catchup with larger DB size are no\n",
      "longer scalable.\n",
      "We present Magma, a hybrid write-optimized storage engine\n",
      "based on LSM Trees [26] and Log-structured storage [28] available\n",
      "in Couchbase 7.1. In this paper, we describe the following key\n",
      "contributions:\n",
      "• Evaluation of copy-on-write B+ tree for high data density\n",
      "• Design of a high data density storage engine that blends\n",
      "LSM Trees with Log-Structured storage to achieve low write\n",
      "amplication\n",
      "• A novel method for garbage collecting the log-structured\n",
      "storage eciently\n",
      "This paper is organized into three parts. We initially discuss the\n",
      "background by providing details on B+Trees in the context of chal-\n",
      "lenges faced by our existing storage engine Couchstore in Section\n",
      "2. Section 3 and 4 discuss the Magma design and our contributions.\n",
      "Section 9 provides experimental evaluation results and discussion.\n",
      "Couchbase distributed database has a microservices approach\n",
      "called multi-dimensional-scaling [6, 9] to horizontally scale all parts\n",
      "of the database. Data service is a distributed high-performance,\n",
      "replicated and elastic key-value document storage service that\n",
      "spans across several nodes as shown in Figure 1. In Couchbase\n",
      "Figure 2: Copy-on-write B+Tree undergoing a modication\n",
      "Couchstore [10] is the current generation storage engine of\n",
      "Couchbase Server for document storage. The overall architecture\n",
      "3497\n",
      "inherits from the storage model of Apache CouchDB [2]. This stor-\n",
      "age engine is battle-hardened in production and has been serving\n",
      "Couchbase customers for almost 10 years. Couchstore is based\n",
      "on copy-on-write (COW) B+Tree and it follows a log-structured\n",
      "append-only storage model. Each vBucket maintains a couchstore\n",
      "le and stores the documents belonging to the vBucket. The le\n",
      "format consists of documents and B+Tree pages interleaved in the\n",
      "le. Each couchstore le maintains three B+Trees, a byKey index\n",
      "for accessing by key, bySeqno index for accessing by seqno, and a\n",
      "metadata B+Tree for storing vBucket statistics and metadata. To\n",
      "look up a document by key, byKey B+Tree is used to obtain the\n",
      "le oset of the document version, and a read is performed from\n",
      "the oset. Couchstore does not have a dedicated managed cache.\n",
      "Rather it depends on the le system buer cache.\n",
      "A copy-on-write B+Tree is an adaptation of B+Tree for the log-\n",
      "structured storage model. Compared to update in-place B+trees, the\n",
      "COW B+Trees can achieve higher write throughput as it performs\n",
      "writes in a sequential access pattern. B+Tree consists of intermedi-\n",
      "ate pages and leaf pages. The leaf page consists of key-record pairs.\n",
      "Intermediate pages consist of key-value pairs with the value being\n",
      "the le osets of pointing pages within the same le.\n",
      "2.2.1 Write Operation. B+Tree modication involves a read-modify-\n",
      "write scheme for the B+Tree page. When a record needs to be added\n",
      "or removed to the COW B+Tree, it locates the leaf page where the\n",
      "record key belongs by traversing the tree from the root page and\n",
      "navigating through the intermediate pages. It makes a copy of the\n",
      "leaf page in memory and modies the page to add or remove the\n",
      "record. The new version of the page is appended to the DB le. Now\n",
      "that the location of the leaf page has changed to a new oset, we\n",
      "need to update the intermediate node that points to the leaf node.\n",
      "Similarly, all the intermediate pages up to the root page need to be\n",
      "rewritten to update the new page locations. As shown in Figure\n",
      "2, if a record is modied or added to page C3, it has to make the\n",
      "modication in C3 and create C3’. Similarly, the pointing parent\n",
      "pages including the intermediate page B2’ and new page A’ need\n",
      "to be written. The older version of the pages (C3, B2, A) become\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "def load_text(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            text = file.read()\n",
    "            print(text[0:10000])\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Please make sure {0} exists.\".format(file_path))\n",
    "        return None\n",
    "    return text \n",
    "text = load_text(\"DataLake/magma.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 21 chunks\n",
      "\n",
      "First chunk sample:\n",
      "ABSTRACT\n",
      "We present Magma, a write-optimized high data density key-value\n",
      "storage engine used in the Couchbase NoSQL distributed docu-\n",
      "ment database. Today’s write-heavy data-intensive applications\n",
      "lik...\n"
     ]
    }
   ],
   "source": [
    "# Text chunking with window and overlap of 500 characters \n",
    "\n",
    "def chunk_text(text, window_size=500, overlap=100):\n",
    "    \"\"\"\n",
    "    Chunk text into overlapping segments of specified window size\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to chunk\n",
    "        window_size (int): Size of each chunk in characters\n",
    "        overlap (int): Number of overlapping characters between chunks\n",
    "        \n",
    "    Returns:\n",
    "        list: List of text chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        # Get chunk of window_size or remaining text if shorter\n",
    "        end = min(start + window_size, len(text))\n",
    "        chunk = text[start:end]\n",
    "        \n",
    "        # Add chunk if it's not empty\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "        # Move start position by window_size - overlap\n",
    "        start = start + window_size - overlap\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "# Create chunks with 500 char window and 100 char overlap\n",
    "data_chunks = chunk_text(text, window_size=1000, overlap=200)\n",
    "\n",
    "print(f\"Created {len(data_chunks)} chunks\")\n",
    "print(f\"\\nFirst chunk sample:\\n{data_chunks[0][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# load the model \n",
    "def load_model(model_name):\n",
    "    \"\"\"\n",
    "    Load a SentenceTransformer model from Hugging Face\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the model to load \n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "    return model\n",
    "\n",
    "model = load_model(\"nomic-ai/CodeRankEmbed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.19513035  1.0389959  -0.6033782  ... -0.22977789  0.12080007\n",
      "   0.56734395]\n",
      " [ 0.5073284  -0.37393796 -0.7079576  ... -0.23452719  1.4435698\n",
      "   0.2964999 ]\n",
      " [ 0.2933281   1.4403888  -0.6809826  ... -0.07078546  0.87308353\n",
      "   0.9864548 ]\n",
      " ...\n",
      " [ 0.60976076  0.2126527  -0.59666383 ...  0.22752672  0.62414616\n",
      "   0.21837918]\n",
      " [ 0.2967483   0.15918534 -0.6006259  ...  0.16863205  0.92862195\n",
      "  -1.0542966 ]\n",
      " [ 0.9830756   1.2716196  -0.39954367 ... -0.49186674 -0.32120115\n",
      "  -1.3642838 ]]\n",
      "torch.Size([21, 21])\n"
     ]
    }
   ],
   "source": [
    "# start the embedding process where the chunks that are created are embedded using the huggingface model \n",
    "\n",
    "def embed_chunks(chunks, model):\n",
    "    \"\"\"\n",
    "    Embed text chunks using the provided model\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): List of text chunks\n",
    "        model: SentenceTransformer model\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Array of embeddings\n",
    "    \"\"\"\n",
    "    sentences = chunks\n",
    "    embeddings = model.encode(sentences)\n",
    "    return embeddings\n",
    "\n",
    "embeddings = embed_chunks(data_chunks,model)\n",
    "print(embeddings)\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector store \n",
    "def create_vector_store(embeddings):\n",
    "    \"\"\"\n",
    "    Create a vector store using FAISS\n",
    "    \n",
    "    Args:\n",
    "        embeddings (np.ndarray): Array of embeddings  \n",
    "    Returns:\n",
    "        faiss.IndexFlatL2: Vector store\n",
    "    \"\"\"\n",
    "\n",
    "    vector_store = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    vector_store.add(embeddings)\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "vector_store = create_vector_store(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e the cost of reads as well as reduce space usage, we have\n",
      "to periodically merge SSTable les and reclaim space. This process\n",
      "is called compaction.\n",
      "A level-based compaction strategy popularized by LevelDB [15,\n",
      "24] is a common compaction strategy for achieving lower read am-\n",
      "plication and space amplication. The LSM Tree is organized into\n",
      "multiple levels of exponentially increasing sizes with the smallest\n",
      "size at the top and the largest being the bottom level. Each level can\n",
      "have several SSTable les. The in-memory component is periodi-\n",
      "cally ushed into level-0 as an SSTable le. Level-0 is a special level\n",
      "that accumulates new data. It can have multiple SSTables with over-\n",
      "lapping key ranges. All other higher levels have non-overlapping\n",
      "key ranges between the SSTables in the level. Each non-level-0 level\n",
      "has a contiguous key range. When level-0 reaches a size threshold,\n",
      "the SSTable les are picked and merged with sstables from the\n",
      "level-1 and the overlapping key range from the level-1\n",
      " where n is the number of\n",
      "items in the SSTable. This can be expensive in terms of CPU andI/O\n",
      "operations.\n",
      "To optimize the lookup, LSM Trees generally maintain a bloom\n",
      "lter per SSTable with high accuracy. This avoids I/O reads from\n",
      "SSTables which do not have the key. Using a bloom-lter with high\n",
      "accuracy, it can service the lookup operation using a single SSTable\n",
      "or a single B+Tree. This makes the cost of lookup similar to that of\n",
      "a traditional B+Tree\n",
      "te-ahead log. The in-memory component uses a sort ordered\n",
      "data structure providing fast lookups and range reads. Once the\n",
      "in-memory component reaches a threshold size limit, it is frozen\n",
      "and a new one is initialized for processing incoming writes. The\n",
      "records from the frozen in-memory component are converted into\n",
      "a B+Tree on a new le. This le is called a sorted strings table\n",
      "(SSTable).\n",
      "2.3.2 Compaction Operation. As the in-memory components are\n",
      "ushed to the disk, more SSTable les are generated. For performing\n",
      "a lookup, it has to search SSTables in the most recent table rst order\n",
      "until the key is found. The I/O and CPU cost becomes proportional\n",
      "3498\n",
      "to the number of tables to be evaluated. A large number of tables\n",
      "also consume space as they may contain stale key-value pairs. To\n",
      "minimize the cost of reads as well as reduce space usage, we have\n",
      "to periodically merge SSTable les and reclaim space. This process\n",
      "is called compaction.\n",
      "A level-based compaction strategy popularized by Lev\n",
      "on-level-0 level\n",
      "has a contiguous key range. When level-0 reaches a size threshold,\n",
      "the SSTable les are picked and merged with sstables from the\n",
      "level-1 and the overlapping key range from the level-1 is replaced\n",
      "with new SSTable les. This involves a k-way merge sort between\n",
      "the source SSTable les. A similar process is followed to manage\n",
      "the size of each level according to the size threshold.\n",
      "Due to the compactions that periodically rewrite the data, LSM\n",
      "Trees can incur high write amplication. For an LSM Tree with a\n",
      "10x level multiplier, each level except the level-0 contributes a write\n",
      "amplication of 10. When data is ushed to level-0 from the write\n",
      "cache, it contributes to a write amplication of 1. Similarly, the\n",
      "write-ahead log also contributes a write amplication of 1. Hence,\n",
      "for an LSM Tree with 5 levels, the worst-case write amplication\n",
      "can go up to 42. For skewed workloads, observed write amplication\n",
      "will be lower than the worst-case amplication.\n",
      "2.3.3 Read Operation. T\n",
      " B+Tree bulk load operation\n",
      "to the new le, rebuilding the B+Tree. While the compaction is\n",
      "running, the writes continue to operate on the DB le. The com-\n",
      "pactor operates on a point-in-time snapshot of the B+Tree. After\n",
      "nishing the B+Tree bulk load, it runs a catchup phase to replay\n",
      "the new additions/deletions that happened to the B+Tree from the\n",
      "point-in-time version used by the compactor up to the latest B+Tree\n",
      "in the DB le. On completion of the catchup phase, the old DB le\n",
      "is removed and writers and readers switch to the new DB le. The\n",
      "space is reclaimed. Compaction is a single-threaded process that\n",
      "runs on a DB le.\n",
      "2.3 Log-Structured Merge Tree\n",
      "Figure 3: LSM Tree architecture\n",
      "LSM Tree is a write-optimized persistent index data structure.\n",
      "LSM Trees achieve high write throughput by utilizing superior\n",
      "sequential write bandwidth of SSDs [18] and spinning disks com-\n",
      "pared to the random I/O access pattern. The large sequential writes\n",
      "are achieved by batching a large number of mutati\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def query_vector_store(query, vector_store, model, data_chunks):\n",
    "    \"\"\"\n",
    "    Query the vector store for the most similar embeddings\n",
    "    \n",
    "    Args:\n",
    "        query (str): Query to search for\n",
    "        vector_store (faiss.IndexFlatL2): Vector store to search\n",
    "        model: SentenceTransformer model\n",
    "        \n",
    "    Returns:\n",
    "        list: List of indices of the most similar embeddings\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query])  # get the embedding\n",
    "    query_embedding = query_embedding.astype('float32')\n",
    "    distances, indices = vector_store.search(query_embedding, k=5)\n",
    "    context = \"\"\n",
    "    for i in indices[0]:\n",
    "        context += data_chunks[i] + \"\\n\"\n",
    "    return context\n",
    "\n",
    "query = \"merging the ss tables , how?\"\n",
    "context = query_vector_store(query, vector_store, model, data_chunks)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test out the embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: merging the ss tables , how?\n",
      "Context: e the cost of reads as well as reduce space usage, we have\n",
      "to periodically merge SSTable les and reclaim space. This process\n",
      "is called compaction.\n",
      "A level-based compaction strategy popularized by LevelDB [15,\n",
      "24] is a common compaction strategy for achieving lower read am-\n",
      "plication and space amplication. The LSM Tree is organized into\n",
      "multiple levels of exponentially increasing sizes with the smallest\n",
      "size at the top and the largest being the bottom level. Each level can\n",
      "have several SSTable les. The in-memory component is periodi-\n",
      "cally ushed into level-0 as an SSTable le. Level-0 is a special level\n",
      "that accumulates new data. It can have multiple SSTables with over-\n",
      "lapping key ranges. All other higher levels have non-overlapping\n",
      "key ranges between the SSTables in the level. Each non-level-0 level\n",
      "has a contiguous key range. When level-0 reaches a size threshold,\n",
      "the SSTable les are picked and merged with sstables from the\n",
      "level-1 and the overlapping key range from the level-1\n",
      " where n is the number of\n",
      "items in the SSTable. This can be expensive in terms of CPU andI/O\n",
      "operations.\n",
      "To optimize the lookup, LSM Trees generally maintain a bloom\n",
      "lter per SSTable with high accuracy. This avoids I/O reads from\n",
      "SSTables which do not have the key. Using a bloom-lter with high\n",
      "accuracy, it can service the lookup operation using a single SSTable\n",
      "or a single B+Tree. This makes the cost of lookup similar to that of\n",
      "a traditional B+Tree\n",
      "te-ahead log. The in-memory component uses a sort ordered\n",
      "data structure providing fast lookups and range reads. Once the\n",
      "in-memory component reaches a threshold size limit, it is frozen\n",
      "and a new one is initialized for processing incoming writes. The\n",
      "records from the frozen in-memory component are converted into\n",
      "a B+Tree on a new le. This le is called a sorted strings table\n",
      "(SSTable).\n",
      "2.3.2 Compaction Operation. As the in-memory components are\n",
      "ushed to the disk, more SSTable les are generated. For performing\n",
      "a lookup, it has to search SSTables in the most recent table rst order\n",
      "until the key is found. The I/O and CPU cost becomes proportional\n",
      "3498\n",
      "to the number of tables to be evaluated. A large number of tables\n",
      "also consume space as they may contain stale key-value pairs. To\n",
      "minimize the cost of reads as well as reduce space usage, we have\n",
      "to periodically merge SSTable les and reclaim space. This process\n",
      "is called compaction.\n",
      "A level-based compaction strategy popularized by Lev\n",
      "on-level-0 level\n",
      "has a contiguous key range. When level-0 reaches a size threshold,\n",
      "the SSTable les are picked and merged with sstables from the\n",
      "level-1 and the overlapping key range from the level-1 is replaced\n",
      "with new SSTable les. This involves a k-way merge sort between\n",
      "the source SSTable les. A similar process is followed to manage\n",
      "the size of each level according to the size threshold.\n",
      "Due to the compactions that periodically rewrite the data, LSM\n",
      "Trees can incur high write amplication. For an LSM Tree with a\n",
      "10x level multiplier, each level except the level-0 contributes a write\n",
      "amplication of 10. When data is ushed to level-0 from the write\n",
      "cache, it contributes to a write amplication of 1. Similarly, the\n",
      "write-ahead log also contributes a write amplication of 1. Hence,\n",
      "for an LSM Tree with 5 levels, the worst-case write amplication\n",
      "can go up to 42. For skewed workloads, observed write amplication\n",
      "will be lower than the worst-case amplication.\n",
      "2.3.3 Read Operation. T\n",
      " B+Tree bulk load operation\n",
      "to the new le, rebuilding the B+Tree. While the compaction is\n",
      "running, the writes continue to operate on the DB le. The com-\n",
      "pactor operates on a point-in-time snapshot of the B+Tree. After\n",
      "nishing the B+Tree bulk load, it runs a catchup phase to replay\n",
      "the new additions/deletions that happened to the B+Tree from the\n",
      "point-in-time version used by the compactor up to the latest B+Tree\n",
      "in the DB le. On completion of the catchup phase, the old DB le\n",
      "is removed and writers and readers switch to the new DB le. The\n",
      "space is reclaimed. Compaction is a single-threaded process that\n",
      "runs on a DB le.\n",
      "2.3 Log-Structured Merge Tree\n",
      "Figure 3: LSM Tree architecture\n",
      "LSM Tree is a write-optimized persistent index data structure.\n",
      "LSM Trees achieve high write throughput by utilizing superior\n",
      "sequential write bandwidth of SSDs [18] and spinning disks com-\n",
      "pared to the random I/O access pattern. The large sequential writes\n",
      "are achieved by batching a large number of mutati\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Query: merging the ss tables , how?\\nContext: e the cost of reads as well as reduce space usage, we have\\nto periodically merge SSTable \\uf022les and reclaim space. This process\\nis called compaction.\\nA level-based compaction strategy popularized by LevelDB [15,\\n24] is a common compaction strategy for achieving lower read am-\\npli\\uf022cation and space ampli\\uf022cation. The LSM Tree is organized into\\nmultiple levels of exponentially increasing sizes with the smallest\\nsize at the top and the largest being the bottom level. Each level can\\nhave several SSTable \\uf022les. The in-memory component is periodi-\\ncally \\uf024ushed into level-0 as an SSTable \\uf022le. Level-0 is a special level\\nthat accumulates new data. It can have multiple SSTables with over-\\nlapping key ranges. All other higher levels have non-overlapping\\nkey ranges between the SSTables in the level. Each non-level-0 level\\nhas a contiguous key range. When level-0 reaches a size threshold,\\nthe SSTable \\uf022les are picked and merged with sstables from the\\nlevel-1 and the overlapping key range from the level-1\\n where n is the number of\\nitems in the SSTable. This can be expensive in terms of CPU andI/O\\noperations.\\nTo optimize the lookup, LSM Trees generally maintain a bloom\\n\\uf022lter per SSTable with high accuracy. This avoids I/O reads from\\nSSTables which do not have the key. Using a bloom-\\uf022lter with high\\naccuracy, it can service the lookup operation using a single SSTable\\nor a single B+Tree. This makes the cost of lookup similar to that of\\na traditional B+Tree\\nte-ahead log. The in-memory component uses a sort ordered\\ndata structure providing fast lookups and range reads. Once the\\nin-memory component reaches a threshold size limit, it is frozen\\nand a new one is initialized for processing incoming writes. The\\nrecords from the frozen in-memory component are converted into\\na B+Tree on a new \\uf022le. This \\uf022le is called a sorted strings table\\n(SSTable).\\n2.3.2 Compaction Operation. As the in-memory components are\\n\\uf024ushed to the disk, more SSTable \\uf022les are generated. For performing\\na lookup, it has to search SSTables in the most recent table \\uf022rst order\\nuntil the key is found. The I/O and CPU cost becomes proportional\\n3498\\nto the number of tables to be evaluated. A large number of tables\\nalso consume space as they may contain stale key-value pairs. To\\nminimize the cost of reads as well as reduce space usage, we have\\nto periodically merge SSTable \\uf022les and reclaim space. This process\\nis called compaction.\\nA level-based compaction strategy popularized by Lev\\non-level-0 level\\nhas a contiguous key range. When level-0 reaches a size threshold,\\nthe SSTable \\uf022les are picked and merged with sstables from the\\nlevel-1 and the overlapping key range from the level-1 is replaced\\nwith new SSTable \\uf022les. This involves a k-way merge sort between\\nthe source SSTable \\uf022les. A similar process is followed to manage\\nthe size of each level according to the size threshold.\\nDue to the compactions that periodically rewrite the data, LSM\\nTrees can incur high write ampli\\uf022cation. For an LSM Tree with a\\n10x level multiplier, each level except the level-0 contributes a write\\nampli\\uf022cation of 10. When data is \\uf024ushed to level-0 from the write\\ncache, it contributes to a write ampli\\uf022cation of 1. Similarly, the\\nwrite-ahead log also contributes a write ampli\\uf022cation of 1. Hence,\\nfor an LSM Tree with 5 levels, the worst-case write ampli\\uf022cation\\ncan go up to 42. For skewed workloads, observed write ampli\\uf022cation\\nwill be lower than the worst-case ampli\\uf022cation.\\n2.3.3 Read Operation. T\\n B+Tree bulk load operation\\nto the new \\uf022le, rebuilding the B+Tree. While the compaction is\\nrunning, the writes continue to operate on the DB \\uf022le. The com-\\npactor operates on a point-in-time snapshot of the B+Tree. After\\n\\uf022nishing the B+Tree bulk load, it runs a catchup phase to replay\\nthe new additions/deletions that happened to the B+Tree from the\\npoint-in-time version used by the compactor up to the latest B+Tree\\nin the DB \\uf022le. On completion of the catchup phase, the old DB \\uf022le\\nis removed and writers and readers switch to the new DB \\uf022le. The\\nspace is reclaimed. Compaction is a single-threaded process that\\nruns on a DB \\uf022le.\\n2.3 Log-Structured Merge Tree\\nFigure 3: LSM Tree architecture\\nLSM Tree is a write-optimized persistent index data structure.\\nLSM Trees achieve high write throughput by utilizing superior\\nsequential write bandwidth of SSDs [18] and spinning disks com-\\npared to the random I/O access pattern. The large sequential writes\\nare achieved by batching a large number of mutati\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now add the context to the query for the LLM\n",
    "\n",
    "def add_context_to_query(query, context):\n",
    "    \"\"\"\n",
    "    Add context to the query for the LLM\n",
    "    \n",
    "    Args:\n",
    "        query (str): Query to add context to\n",
    "        context (str): Context to add to the query\n",
    "        \n",
    "    Returns:\n",
    "        str: Query with context added\n",
    "    \"\"\"\n",
    "    return f\"Query: {query}\\nContext: {context}\"\n",
    "\n",
    "query_with_context = add_context_to_query(query, context)\n",
    "print(query_with_context)\n",
    "# now use the LLM to answer the query \n",
    "\n",
    "\n",
    "add_context_to_query(query, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to understand how merging SSTables works in an LSM Tree, specifically using a level-based compaction strategy like the one used in LevelDB. From what I gather, SSTables are these sorted string tables that store data on disk, and over time they accumulate as writes come into the system. The problem is that having too many SSTables can make read operations slow because you have to check each one until you find the key. Also, all those SSTables take up a lot of space.\n",
      "\n",
      "So, compaction is the process where we merge these SSTables to reduce their number and reclaim unused space. I remember reading that in LevelDB, they organize these SSTables into levels, starting from level 0 which is kind of a special case. Each subsequent level has larger SSTables with non-overlapping key ranges. The idea is that higher levels have less frequent changes because data moves down the levels over time.\n",
      "\n",
      "When level 0 gets too full (I think it's when the number of SSTables or their total size exceeds a certain threshold), the compaction process kicks in. The first step is to merge some of these SSTables from level 0 with those in level 1. But how exactly does this merging work? I'm picturing that all the data from multiple SSTables in level 0 and maybe part of level 1 are combined into new, larger SSTables.\n",
      "\n",
      "The process must involve reading all the data from the selected SSTables, sorting them (since each is already sorted but might have overlapping keys), and then writing a new SSTable. This sounds like a merge sort operation, where multiple sorted lists are merged into one. So it's a k-way merge, where k is the number of SSTables being merged at that time.\n",
      "\n",
      "I'm also thinking about how this affects read operations during compaction. Since the data is being rewritten, any reads happening while compaction is running might have to handle temporary files or point-in-time snapshots. There must be some mechanism to ensure consistency, maybe by freezing writes for a short period or using a checkpointing process.\n",
      "\n",
      "Another thing I'm curious about is how the levels are managed after compaction. Once level 0's SSTables are merged into level 1, those original level 0 files are probably deleted or marked as obsolete so that future reads can ignore them. This helps in reducing the number of files and thus the overhead during lookups.\n",
      "\n",
      "I also recall something about write amplification. Each time data is moved from one level to another, it's written again, which multiplies the total writes beyond the initial data size. For example, if each level has a 10x multiplier and there are five levels, the worst-case write amplification could be quite high. But I'm not entirely clear on how exactly this multiplication works—maybe each level requires multiple passes or something.\n",
      "\n",
      "Wait, so when a new SSTable is created from merging lower levels, it's written to a higher level, right? So data that was in level 0 and then moved to level 1 would have been written once as level 0, then again during compaction into level 1. Each time it's promoted to a higher level, it incurs another write.\n",
      "\n",
      "I'm also trying to visualize the structure. Level 0 can have many SSTables with overlapping ranges because they're newly flushed from memory. As soon as they reach a certain size, some of them get merged into level 1. Level 1 has non-overlapping SSTables, so each file in level 1 covers a unique range. When level 1 gets full, the same process happens—merge with level 2, and so on.\n",
      "\n",
      "This must mean that higher levels are less frequently accessed because their data is older and more stable. So during a read operation, the system first checks the most recent SSTables (probably in lower levels) before moving up to higher levels, which have larger, less frequently changing data.\n",
      "\n",
      "I'm also thinking about how this affects performance under different workloads. For example, if there are a lot of writes, level 0 can fill up quickly, leading to more frequent compactions. This might increase the CPU and I/O usage during compaction but improve read performance because it reduces the number of files that need to be checked.\n",
      "\n",
      "Another point is about concurrency—can multiple compactions happen at the same time, or is it a single-threaded process? From what I remember, LevelDB's compaction is single-threaded for a particular file, which might mean that during peak times, compaction could become a bottleneck if not managed properly.\n",
      "\n",
      "I'm also wondering about how the system decides which SSTables to merge. Is it based on size, number of files, or some other metric? I think in LevelDB, the number of files in level 0 is monitored, and once it passes a certain threshold (like 4?), compaction starts. This makes sense because too many small files can cause read performance issues.\n",
      "\n",
      "In terms of implementation details, when merging, each SSTable from the lower levels is read entirely, sorted, and then written into new files. Since each SSTable is already sorted, the merge process is efficient—it just needs to interleave the data correctly without overlapping keys.\n",
      "\n",
      "I'm also thinking about how this affects storage space. Each compaction writes new data, which might require temporary space. But since the old SSTables are eventually deleted or marked as obsolete, the net space used should decrease over time, assuming that not all data is being written infinitely and some can be compacted away.\n",
      "\n",
      "Another consideration is the impact on read operations during compaction. If a compaction is ongoing, does it affect how reads access the data? I believe that in LevelDB, during compaction, reads are served from both the new and old files until the compaction process completes, after which the old files are discarded. This ensures that reads don't have to wait for compaction to finish before accessing new data.\n",
      "\n",
      "I'm also trying to connect this with the concept of memtables. When a write comes in, it's first written to a memtable in memory. Once the memtable is full, it's flushed to an SSTable on disk. So the initial level 0 is built from these flushes. As more writes come in, new memtables are created, and the old ones are either promoted or compacted.\n",
      "\n",
      "Wait, so when a compaction happens, it's not just merging level 0 with level 1; sometimes, if multiple levels are involved, data can be merged across several levels at once? Or does each level handle its own merging independently?\n",
      "\n",
      "I think in LevelDB, the compaction process for a particular file only merges up to the next level (e.g., from level 0 to 1, or level k to k+1). The idea is that higher levels are larger and less frequently modified, so their files remain stable.\n",
      "\n",
      "Another point is about the size of the SSTables in each level. Each subsequent level has SSTables that are exponentially larger than the previous one. So level 0 might have small files (say, 2MB each), level 1 could be 16MB, level 2 even bigger, and so on. This exponential growth ensures that higher levels don't require as many files, reducing the overhead of managing them.\n",
      "\n",
      "I'm also thinking about how the compaction process affects write amplification. Every time data is moved from one level to another, it's written again, which increases the total number of writes beyond the initial data. For example, if each SSTable in level 0 is merged into a larger one in level 1, and then that gets promoted further, each step adds more writes.\n",
      "\n",
      "But I'm not sure how this compares to other storage systems or if there are optimizations to reduce write amplification. Maybe newer LSM implementations have strategies to minimize this by better managing the compaction process or using different leveling strategies.\n",
      "\n",
      "Another thing is about the trade-offs between compaction and read/write performance. If you have too many levels, each read might take longer because it has to check more files across multiple levels. On the other hand, if you have fewer levels, compaction could be more frequent but might reduce the number of files per level.\n",
      "\n",
      "I'm also trying to think about how this works in practice with real-world data. Suppose I have a dataset that's being written to heavily—how does the compaction process keep up without causing too much overhead? It must balance the need for efficient writes against the cost of frequent compactions.\n",
      "\n",
      "In summary, merging SSTables in an LSM Tree using level-based compaction involves monitoring the number or size of files in each level, triggering compaction when a threshold is reached, and then performing a k-way merge of the selected SSTables to create new, larger files in the next level. This process reduces the number of files and improves read performance by minimizing the number of files that need to be checked during a lookup. However, it does come with the trade-off of increased write operations due to write amplification.\n",
      "\n",
      "I think I have a basic understanding now, but there are still some details I'm fuzzy on, like exactly how compaction triggers are set up and how the system handles concurrent writes and reads during compaction. Maybe looking into LevelDB's documentation or source code would clarify these points.\n",
      "</think>\n",
      "\n",
      "Merging SSTables in an LSM Tree using level-based compaction is a sophisticated process designed to optimize read and write operations. Here's a detailed explanation of how it works:\n",
      "\n",
      "### Key Components:\n",
      "1. **Memtables and SSTables**:\n",
      "   - Writes are initially stored in memtables (in-memory structures). When a memtable reaches capacity, it is flushed to disk as an SSTable.\n",
      "   - SSTables are immutable; once written, they cannot be modified.\n",
      "\n",
      "2. **Levels**:\n",
      "   - The storage hierarchy consists of multiple levels (0, 1, 2, ...). Each level contains SSTables of increasing size and stability.\n",
      "   - Level 0 holds the most recent data in smaller files, while higher levels contain older data in larger, more stable files.\n",
      "\n",
      "3. **Compaction Triggers**:\n",
      "   - Compaction is triggered when a certain number of files accumulate in level 0 (e.g., 4 or more), ensuring that read operations remain efficient by reducing the number of files accessed.\n",
      "\n",
      "### Process Overview:\n",
      "\n",
      "1. **Flush to Level 0**:\n",
      "   - When a memtable is full, it is flushed to disk as an SSTable and added to level 0.\n",
      "\n",
      "2. **Compaction Initiation**:\n",
      "   - If level 0 contains more than a configured number of SSTables (e.g., 4), compaction begins.\n",
      "   - The goal is to merge all SSTables in level 0 into larger files in the next level (level 1).\n",
      "\n",
      "3. **Merging SSTables**:\n",
      "   - A single-threaded process merges all SSTables from level 0 with the smallest file in level 1 (if it exists) into a new, larger SSTable in level 1.\n",
      "   - This merge operation is efficient since both sets of data are already sorted.\n",
      "\n",
      "4. **Repeat for Higher Levels**:\n",
      "   - The compaction process continues up through each level as needed. Each level's files are merged to form even larger files in the subsequent level.\n",
      "\n",
      "5. **File Management**:\n",
      "   - Once an SSTable from a lower level is merged into a higher level, it is marked for deletion. Old files are periodically removed to free up space.\n",
      "\n",
      "6. **Read Operations**:\n",
      "   - Reads access data across all levels, starting from the lowest level and moving upwards until all required data is retrieved.\n",
      "   - As compaction progresses, the number of files accessed during reads decreases, improving performance.\n",
      "\n",
      "### Benefits:\n",
      "\n",
      "- **Efficient Writes**: LSM Trees excel at handling write-heavy workloads by writing new data to memtables and flushing them to disk efficiently.\n",
      "- **Reduced Read Latency**: By merging smaller SSTables into larger ones, compaction reduces the number of files that need to be read during lookups, lowering latency.\n",
      "- **Scalability**: The hierarchical structure allows for efficient storage of large datasets, making LSM Trees suitable for high-scale applications.\n",
      "\n",
      "### Challenges:\n",
      "\n",
      "- **Write Amplification**: Each time data is compacted, it results in multiple writes, increasing the overall I/O overhead.\n",
      "- **Compaction Overhead**: Frequent compactions can lead to increased CPU and disk usage, potentially becoming a bottleneck under heavy write loads.\n",
      "\n",
      "### Summary:\n",
      "\n",
      "In an LSM Tree with level-based compaction, SSTables are merged into larger files as they move up through levels. This process ensures efficient reads by minimizing the number of files accessed and supports high write throughput by writing to memtables and flushing them to disk. While this approach offers significant benefits for certain workloads, it also introduces trade-offs in terms of write amplification and compaction overhead, which must be managed carefully for optimal performance.\n",
      "\n",
      "This explanation provides a foundational understanding of how LSM Trees handle data merging through compaction, balancing the needs for efficient storage and retrieval operations."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Streaming complete'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# send the query to the LLM \n",
    "def get_ollama_suggestions(query_with_context):\n",
    "    response = ollama.chat(model='deepseek-r1:14b', messages=[\n",
    "        {\n",
    "        'role': 'user',\n",
    "        'content': query_with_context\n",
    "        },\n",
    "    ],  options={\"temperature\": 0.8}, stream=True )\n",
    "    \n",
    "    for chunk in response:\n",
    "        print(chunk[\"message\"][\"content\"], end='', flush=True)\n",
    "        \n",
    "    return \"Streaming complete\"\n",
    "\n",
    "get_ollama_suggestions(query_with_context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
