{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub\n",
    "!pip install 'smolagents[litellm]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from smolagents import HfApiModel, LiteLLMModel, TransformersModel, tool\n",
    "from smolagents.agents import CodeAgent, ToolCallingAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Chose model: '{chosen_inference}'\")\n",
    "\n",
    "if chosen_inference == \"hf_api\":\n",
    "    model = HfApiModel(model_id=\"meta-llama/Llama-3.3-70B-Instruct\")\n",
    "\n",
    "elif chosen_inference == \"transformers\":\n",
    "    model = TransformersModel(model_id=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\", device_map=\"auto\", max_new_tokens=1000)\n",
    "\n",
    "elif chosen_inference == \"ollama\":\n",
    "    model = LiteLLMModel(\n",
    "        model_id=\"ollama_chat/llama3.2\",\n",
    "        api_base=\"http://localhost:11434\",  # replace with remote open-ai compatible server if necessary\n",
    "        api_key=\"your-api-key\",  # replace with API key if necessary\n",
    "        num_ctx=8192,  # ollama default is 2048 which will often fail horribly. 8192 works for easy tasks, more is better. Check https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator to calculate how much VRAM this will need for the selected model.\n",
    "    )\n",
    "\n",
    "elif chosen_inference == \"litellm\":\n",
    "    # For anthropic: change model_id below to 'anthropic/claude-3-5-sonnet-latest'\n",
    "    model = LiteLLMModel(model_id=\"gpt-4o\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str, celsius: Optional[bool] = False) -> str:\n",
    "    \"\"\"\n",
    "    Get weather in the next days at given location.\n",
    "    Secretly this tool does not care about the location, it hates the weather everywhere.\n",
    "\n",
    "    Args:\n",
    "        location: the location\n",
    "        celsius: the temperature\n",
    "    \"\"\"\n",
    "    return \"The weather is really with torrential rains and temperatures below -10Â°C\"\n",
    "\n",
    "\n",
    "agent = ToolCallingAgent(tools=[get_weather], model=model)\n",
    "\n",
    "print(\"ToolCallingAgent:\", agent.run(\"What's the weather like in Paris?\"))\n",
    "\n",
    "agent = CodeAgent(tools=[get_weather], model=model)\n",
    "\n",
    "print(\"CodeAgent:\", agent.run(\"What's the weather like in Paris?\"))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
